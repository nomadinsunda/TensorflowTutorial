{"cells":[{"cell_type":"markdown","metadata":{"id":"Tce3stUlHN0L"},"source":["##### Copyright 2020 The TensorFlow Authors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuOe1ymfHZPu"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"qFdPvlXBOdUN"},"source":["# Introduction to gradients and automatic differentiation"]},{"cell_type":"markdown","metadata":{"id":"MfBg1C5NB3X0"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"r6P32iYYV27b"},"source":["## Automatic Differentiation and Gradients\n","\n","[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n","is useful for implementing machine learning algorithms such as\n","[backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for training\n","neural networks.\n","\n","In this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution."]},{"cell_type":"markdown","metadata":{"id":"MUXex9ctTuDB"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"IqR2PQG4ZaZ0","executionInfo":{"status":"ok","timestamp":1675900835742,"user_tz":-540,"elapsed":3307,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf"]},{"cell_type":"code","source":["tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"0u_aqmCh3bYK","executionInfo":{"status":"ok","timestamp":1675900919850,"user_tz":-540,"elapsed":259,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"1e84dcdf-5539-49f2-b8bd-6fbf3efe4f64"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.9.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"xHxb-dlhMIzW"},"source":["## Computing gradients\n","\n","To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the *forward* pass.  Then, during the *backward pass*, TensorFlow traverses this list of operations in reverse order to compute gradients."]},{"cell_type":"markdown","source":["자동으로 미분하기 위해 TensorFlow는 정방향 패스 중에 어떤 Ops이 어떤 순서로 발생하는지 기억해야 합니다. 그런 다음 역방향 패스 중에 TensorFlow는 이 Ops 목록을 역순으로 탐색하여 기울기를 계산합니다."],"metadata":{"id":"1Xu1r41zckhE"}},{"cell_type":"markdown","metadata":{"id":"1CLWJl0QliB0"},"source":["## Gradient tapes\n","\n","TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variable`s.\n","TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n","\n","Here is a simple example:"]},{"cell_type":"markdown","source":["TensorFlow는 자동 미분을 위해 tf.GradientTape API를 제공합니다. 즉, 일반적으로 tf.Variables와 같은 일부 입력에 대한 계산의 gradients를 계산합니다. TensorFlow는 tf.GradientTape 컨텍스트 내에서 실행된 관련 Ops을 \"테이프\"에 \"기록\"합니다. 그런 다음 TensorFlow는 해당 테이프를 사용하여 `reverse 모드 미분`을 사용하여 \"기록된\" 계산의 gradients를 계산합니다.\n","\n","다음은 간단한 예입니다."],"metadata":{"id":"kiYZPgc4cyFG"}},{"cell_type":"code","source":["xx = 4\n","yy = xx**3\n","print(yy)"],"metadata":{"id":"USOg35Szdmoc","outputId":"a6e8898f-02c2-44ff-a878-511b065d1a34","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675901073930,"user_tz":-540,"elapsed":269,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["64\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Xq9GgTCP7a4A","executionInfo":{"status":"ok","timestamp":1675901119815,"user_tz":-540,"elapsed":249,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"outputs":[],"source":["x = tf.Variable(3.0)\n","\n","with tf.GradientTape() as tape: # current persistent[default:false]\n","  y = x**2  # -> y' = 2x"]},{"cell_type":"markdown","metadata":{"id":"CR9tFAP_7cra"},"source":["Once you've recorded some operations, use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables):"]},{"cell_type":"markdown","source":["일부 Ops을 기록했으면 `GradientTape.gradient(target, sources)`를 사용하여 일부 소스(종종 모델의 변수)에 대한 일부 타겟(종종 loss)의 기울기를 계산합니다."],"metadata":{"id":"D8MCTX2PdKAi"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"LsvrwF6bHroC","outputId":"8b7809a4-d356-43f5-8cf4-70b75220c7a9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675901202699,"user_tz":-540,"elapsed":256,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(6.0, shape=(), dtype=float32)\n"]}],"source":["# dy = 2x * dx\n","dy_dx = tape.gradient(y, x)\n","dy_dx.numpy()\n","print(dy_dx)"]},{"cell_type":"code","source":["dy_dx = tape.gradient(y, x)"],"metadata":{"id":"p0HenCPJIe5T","colab":{"base_uri":"https://localhost:8080/","height":314},"executionInfo":{"status":"error","timestamp":1675901235545,"user_tz":-540,"elapsed":720,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"67ebd2ec-9757-48d9-909b-6e0f41028209"},"execution_count":6,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-aeb59fe979f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdy_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \"\"\"\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n\u001b[0m\u001b[1;32m   1041\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"]}]},{"cell_type":"markdown","metadata":{"id":"Q2_aqsO25Vx1"},"source":["The above example uses scalars, but `tf.GradientTape` works as easily on any tensor:"]},{"cell_type":"markdown","source":["위의 예는 스칼라(소스가 스칼라)를 사용하지만 tf.GradientTape는 모든 텐서에서 쉽게 작동합니다."],"metadata":{"id":"4-vcI1VZd9FE"}},{"cell_type":"code","source":["meanx = tf.constant([[1., 1.], [2., 2.]])\n","tf.reduce_mean(meanx)"],"metadata":{"id":"kg6ZN11BeKnR","outputId":"3e785727-8ead-46c2-9e7f-33c5c9c2bd88","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675901316826,"user_tz":-540,"elapsed":260,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=1.5>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["w = tf.Variable(tf.random.normal((3, 2)), name='w')\n","b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n","x = [[1., 2., 3.]]"],"metadata":{"id":"wVc8949OgesT","executionInfo":{"status":"ok","timestamp":1675901400631,"user_tz":-540,"elapsed":277,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vacZ3-Ws5VdV","executionInfo":{"status":"ok","timestamp":1675901449286,"user_tz":-540,"elapsed":262,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"outputs":[],"source":["with tf.GradientTape(persistent=True) as tape: #persistent[default:False]\n","  y = x @ w + b\n","  loss = tf.reduce_mean(y**2)"]},{"cell_type":"markdown","metadata":{"id":"i4eXOkrQ-9Pb"},"source":["To get the gradient of `loss` with respect to both variables, you can pass both as sources to the `gradient` method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see `tf.nest`)."]},{"cell_type":"markdown","source":["두 변수에 대한 손실 기울기를 얻으려면 둘 다 `gradient` 메소드에 소스로 전달할 수 있습니다. 테이프는 소스가 전달되는 방식에 대해 유연하며 `list` 또는 `dictionary`의 중첩된 조합을 허용하고 동일한 방식으로 구조화된 그래디언트를 반환합니다(tf.nest 참조)."],"metadata":{"id":"qrgXSNGtegIW"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"luOtK1Da_BR0","executionInfo":{"status":"ok","timestamp":1675901469489,"user_tz":-540,"elapsed":268,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"314e5727-c35b-4b99-c655-aeea27cf6f6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 2.935216  -1.7986858]\n"," [ 5.870432  -3.5973716]\n"," [ 8.805648  -5.396057 ]], shape=(3, 2), dtype=float32)\n","tf.Tensor([ 2.935216  -1.7986858], shape=(2,), dtype=float32)\n"]}],"source":["[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n","print(dl_dw)\n","print(dl_db)"]},{"cell_type":"markdown","metadata":{"id":"Ei4iVXi6qgM7"},"source":["The gradient with respect to each source has the shape of the source:"]},{"cell_type":"markdown","source":["각 소스에 대한 gradient는 소스의 shape을 갖습니다."],"metadata":{"id":"cPgsABEugTX9"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"aYbWRFPZqk4U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675901603663,"user_tz":-540,"elapsed":243,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"46932ba8-7510-41eb-c326-b99f3004ec29"},"outputs":[{"output_type":"stream","name":"stdout","text":["(3, 2)\n","(3, 2)\n"]}],"source":["print(w.shape)\n","print(dl_dw.shape)"]},{"cell_type":"markdown","metadata":{"id":"dI_SzxHsvao1"},"source":["Here is the gradient calculation again, this time passing a dictionary of variables:"]},{"cell_type":"markdown","source":["다음은 다시 gradient 계산입니다. 이번에는 dictionary 타입의 변수를 전달합니다."],"metadata":{"id":"zV3pegypjNwj"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"d73cY6NOuaMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675901628103,"user_tz":-540,"elapsed":245,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"7abe4c03-1ad1-482a-f52c-8d5f2442434c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ 2.935216  -1.7986858]\n"," [ 5.870432  -3.5973716]\n"," [ 8.805648  -5.396057 ]], shape=(3, 2), dtype=float32)\n","tf.Tensor([ 2.935216  -1.7986858], shape=(2,), dtype=float32)\n"]}],"source":["my_vars = {\n","    'w': w,\n","    'b': b\n","}\n","\n","grad = tape.gradient(loss, my_vars)\n","print(grad['w'])\n","print(grad['b'])"]},{"cell_type":"code","source":["del tape"],"metadata":{"id":"kIcjrxW3JsRK","executionInfo":{"status":"ok","timestamp":1675901687983,"user_tz":-540,"elapsed":259,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["grad = tape.gradient(loss, my_vars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"MSp_zjx2hnHz","executionInfo":{"status":"error","timestamp":1675901697934,"user_tz":-540,"elapsed":268,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"2559f7fb-aec9-4972-d877-9b7330e690bf"},"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-96a3a139bb4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'tape' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"HZ2LvHifEMgO"},"source":["## Gradients with respect to a model\n","\n","It's common to collect `tf.Variables` into a `tf.Module` or one of its subclasses (`layers.Layer`, `keras.Model`) for [checkpointing](checkpoint.ipynb) and [exporting](saved_model.ipynb).\n","\n","In most cases, you will want to calculate gradients with respect to a model's trainable variables.  Since all subclasses of `tf.Module` aggregate their variables in the `Module.trainable_variables` property, you can calculate these gradients in a few lines of code: "]},{"cell_type":"markdown","source":["checkpointing 및 exporting를 위해 tf.Variables를 tf.Module 또는 tf.Module 하위 클래스(layers.Layer, keras.Model) 중 하나로 collect하는 것이 일반적입니다.\n","\n","대부분의 경우 모델의 학습 가능한 변수에 대한 gradient를 계산하려고 합니다. **tf.Module의 모든 하위 클래스는 Module.trainable_variables 속성에서 변수를 집계(aggregate)**하므로 몇 줄의 코드로 이러한 gradient를 계산할 수 있습니다."],"metadata":{"id":"Z2c8rWZnjlWD"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"JvesHtbQESc-","executionInfo":{"status":"ok","timestamp":1675903713742,"user_tz":-540,"elapsed":258,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"outputs":[],"source":["layer = tf.keras.layers.Dense(2, activation='relu') # Dense는 keras에서 지원해주는 hidden layer[Full connected layer]\n","# 현재, layer의 weight 파라미터의 갯수를 알 수 있는가?\n","# 그러나 bias의 갯수는 알 수 있다\n","\n","x = tf.constant([[1., 2., 3.]])\n","\n","with tf.GradientTape() as tape:\n","  # Forward pass\n","  y = layer(x)\n","  # layer.trainable = False\n","  # 이제 layer의 weight 파라미터 갯수를 알 수 있다.\n","  # print(\"y=\", y)\n","  loss = tf.reduce_mean(y**2)  # MSE 흉내!!!"]},{"cell_type":"code","source":["print(layer.trainable_variables)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tSHBG-8sLQHE","executionInfo":{"status":"ok","timestamp":1675904848697,"user_tz":-540,"elapsed":807,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"1267f36f-c022-46c3-8fa5-8bb15588bda2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[<tf.Variable 'dense_1/kernel:0' shape=(3, 2) dtype=float32, numpy=\n","array([[ 0.19564474, -0.61278117],\n","       [ 0.7937056 ,  0.7611126 ],\n","       [-0.0087049 , -0.31788695]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n"]}]},{"cell_type":"code","source":["# layer.trainable = False # Gradient Tape context 외부에서 설정을 해도 적용이 된다"],"metadata":{"id":"dSPWaKPkMxtz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate gradients with respect to every trainable variable\n","grad = tape.gradient(loss, layer.trainable_variables)\n","print(grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"riXd1qEkmhMT","executionInfo":{"status":"ok","timestamp":1675905021805,"user_tz":-540,"elapsed":263,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"5bd72f9a-34d6-4bc7-bbf8-e7ee5bd5a5aa"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n","array([[1.7569412, 0.       ],\n","       [3.5138824, 0.       ],\n","       [5.2708235, 0.       ]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.7569412, 0.       ], dtype=float32)>]\n"]}]},{"cell_type":"markdown","source":["zip() 기본 문법"],"metadata":{"id":"xD7ltqkrkhUr"}},{"cell_type":"markdown","source":["zip() 함수는 여러 개의 순회 가능한(iterable) 객체를 인자로 받고, 각 객체가 담고 있는 원소를 tuple의 형태로 차례로 접근할 수 있는 반복자(iterator)를 반환"],"metadata":{"id":"mpSA4vRGkjj8"}},{"cell_type":"code","source":["numbers = [1, 2, 3]\n","letters = [\"A\", \"B\", \"C\"]\n","for pair in zip(numbers, letters):\n","  print(pair)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1-vj9LGkoW8","executionInfo":{"status":"ok","timestamp":1675905174966,"user_tz":-540,"elapsed":275,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"edf7444a-d548-4800-b159-4b5aeb3f483c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 'A')\n","(2, 'B')\n","(3, 'C')\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{"id":"PR_ezr6UFrpI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675905209358,"user_tz":-540,"elapsed":474,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"6a9ed08a-666a-4396-e583-7f0b1801d367"},"outputs":[{"output_type":"stream","name":"stdout","text":["dense_1/kernel:0, shape: (3, 2)\n","dense_1/bias:0, shape: (2,)\n"]}],"source":["for var, g in zip(layer.trainable_variables, grad):\n","  print(f'{var.name}, shape: {g.shape}')"]},{"cell_type":"markdown","metadata":{"id":"f6Gx6LS714zR"},"source":["<a id=\"watches\"></a>\n","\n","## Controlling what the tape watches"]},{"cell_type":"markdown","metadata":{"id":"N4VlqKFzzGaC"},"source":["The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n","\n","* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n","* The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n","* The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n","\n","For example, the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"]},{"cell_type":"markdown","source":["기본 동작은 **trainable** tf.Variable에 액세스한 후 모든 Ops을 기록하는 것입니다. 그 이유는 다음과 같습니다.\n","* 테이프는 역방향 패스의 기울기를 계산하기 위해 순방향 패스에 기록할 오퍼레이션들을 알아야 합니다.\n","* 테이프에는 중간 출력에 대한 참조가 있으므로 불필요한 오퍼레이션들을 기록하고 싶지 않습니다.\n","* 가장 일반적인 사용 사례는 모델의 모든 학습 가능한 변수에 대한 loss의 기울기를 계산하는 것입니다.\n","\n","예를 들어, 다음 코드은 `tf.Tensor`가 기본적으로 \"감시\"가 되지 않고 `tf.Variable`의 `trainable` 속성이 `False`이기 때문에 그라디언트를 계산하는 데 실패합니다."],"metadata":{"id":"50ZW3tuBos9A"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"Kj9gPckdB37a","executionInfo":{"status":"ok","timestamp":1675905850649,"user_tz":-540,"elapsed":257,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"outputs":[],"source":["# A trainable variable\n","x0 = tf.Variable(3.0, name='x0')\n","\n","# Not trainable\n","# tf.Variable의 trainable 속성이 False이기 때문에 그라디언트를 계산하는 데 실패합니다.\n","x1 = tf.Variable(3.0, name='x1', trainable=False)\n","\n","# Not a Variable: A variable + tensor returns a tensor.\n","# x2 = tf.Variable(2.0, name='x2') + 1.0\n","x2 = tf.Variable(2.0, name='x2')\n","# print(\"x2=\", x2)\n","\n","# Not a variable\n","x3 = tf.constant(3.0, name='x3')"]},{"cell_type":"code","source":["with tf.GradientTape() as tape:\n","  # tape.watch(x2) # GradientTape의 watched_variables 속성에 반영되지 않는다\n","  y = (x0**2) + (x1**2) + (x2**2)"],"metadata":{"id":"oZgT0ddiOZB8","executionInfo":{"status":"ok","timestamp":1675905871528,"user_tz":-540,"elapsed":1,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["grad = tape.gradient(y, [x0, x1, x2, x3])\n","\n","for g in grad:\n","  print(\"g=\", g)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvIGEpIExhmp","executionInfo":{"status":"ok","timestamp":1675905889427,"user_tz":-540,"elapsed":246,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"54402f01-2ffa-482c-8976-589bc7af8b26"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["g= tf.Tensor(6.0, shape=(), dtype=float32)\n","g= None\n","g= tf.Tensor(4.0, shape=(), dtype=float32)\n","g= None\n"]}]},{"cell_type":"markdown","metadata":{"id":"RkcpQnLgNxgi"},"source":["You can list the variables being watched by the tape using the `GradientTape.watched_variables` method:"]},{"cell_type":"markdown","source":["GradientTape.watched_variables 메서드를 사용하여 테이프에서 감시 중인 변수를 나열할 수 있습니다."],"metadata":{"id":"FnI_VFXUvY7y"}},{"cell_type":"code","source":["for var in tape.watched_variables():\n","  print(var.name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HHfzqNMwPntM","executionInfo":{"status":"ok","timestamp":1675905994725,"user_tz":-540,"elapsed":279,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"676c86d8-87e5-4cda-cec3-825adfd58a19"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["x0:0\n","x2:0\n"]}]},{"cell_type":"code","execution_count":25,"metadata":{"id":"hwNwjW1eAkib","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675906044219,"user_tz":-540,"elapsed":256,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"b27bb9bb-8d15-4382-bbc3-151d7ce59921"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['x0:0', 'x2:0']"]},"metadata":{},"execution_count":25}],"source":["[var.name for var in tape.watched_variables()]"]},{"cell_type":"markdown","metadata":{"id":"NB9I1uFvB4tf"},"source":["`tf.GradientTape` provides hooks that give the user control over what is or is not watched.\n","\n","To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`:"]},{"cell_type":"markdown","source":["tf.GradientTape는 사용자가 감시 여부를 제어할 수 있는 hook를 제공합니다.</br>\n","tf.Tensor에 대한 그라디언트를 기록하려면 GradientTape.watch(x)를 호출해야 합니다."],"metadata":{"id":"aWpa4NfivejM"}},{"cell_type":"code","execution_count":26,"metadata":{"id":"tVN1QqFRDHBK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675906276226,"user_tz":-540,"elapsed":249,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"7a41515c-9431-4cdb-cd29-ca30eafbda1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["6.0\n"]}],"source":["x = tf.constant(3.0)\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = x**2\n","\n","# dy = 2x * dx\n","dy_dx = tape.gradient(y, x)\n","print(dy_dx.numpy())"]},{"cell_type":"code","source":["x = tf.constant(3.0)\n","with tf.GradientTape() as tape:\n","  y = x**2\n","\n","# dy = 2x * dx\n","dy_dx = tape.gradient(y, x)\n","print(dy_dx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xt-okZCXGNn_","executionInfo":{"status":"ok","timestamp":1675906289417,"user_tz":-540,"elapsed":278,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"d190f355-3527-4005-9324-004bb8d206bb"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"markdown","metadata":{"id":"qxsiYnf2DN8K"},"source":["Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"]},{"cell_type":"markdown","source":["<font color=\"red\">반대로 감시하고 있는 모든 tf.Variables 디폴트 동작을 비활성화하려면</font> 그래디언트 테이프를 생성할 때 **watch_accessed_variables=False**로 설정하십시오. 이 계산은 두 개의 변수를 사용하지만 변수 중 하나에 대한 gradient만 연결합니다."],"metadata":{"id":"C8VUyty0v9Cy"}},{"cell_type":"code","execution_count":28,"metadata":{"id":"7QPzwWvSEwIp","executionInfo":{"status":"ok","timestamp":1675906373144,"user_tz":-540,"elapsed":255,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"outputs":[],"source":["x0 = tf.Variable(0.0)\n","x1 = tf.Variable(10.0)\n","\n","with tf.GradientTape(watch_accessed_variables=False) as tape:\n","  tape.watch(x1)\n","  y0 = tf.math.sin(x0)\n","  y1 = tf.nn.softplus(x1)\n","  y = y0 + y1\n","  ys = tf.reduce_sum(y)"]},{"cell_type":"markdown","metadata":{"id":"TRduLbE1H2IJ"},"source":["Since `GradientTape.watch` was not called on `x0`, no gradient is computed with respect to it:"]},{"cell_type":"markdown","source":["'x0'에 대해 'GradientTape.watch'가 호출되지 않았으므로 이와 관련된 그라디언트가 계산되지 않습니다."],"metadata":{"id":"fcW8xJWhwh6W"}},{"cell_type":"code","execution_count":29,"metadata":{"id":"e6GM-3evH1Sz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675906459057,"user_tz":-540,"elapsed":249,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"dee97ad9-4da9-43f2-e070-54120afe0133"},"outputs":[{"output_type":"stream","name":"stdout","text":["dy/dx0: None\n","dy/dx1: tf.Tensor(0.9999546, shape=(), dtype=float32)\n"]}],"source":["# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n","grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n","\n","print('dy/dx0:', grad['x0'])\n","print('dy/dx1:', grad['x1'])\n","# print('dy/dx1:', grad['x1'].numpy())"]},{"cell_type":"markdown","metadata":{"id":"2g1nKB6P-OnA"},"source":["## Intermediate results\n","\n","You can also request gradients of the output with respect to intermediate values computed inside the `tf.GradientTape` context."]},{"cell_type":"markdown","source":["또한 tf.GradientTape 컨텍스트 내에서 계산된 중간 값에 대한 출력 기울기를 요청할 수 있습니다."],"metadata":{"id":"qgUFuTsAxCll"}},{"cell_type":"code","execution_count":30,"metadata":{"id":"7XaPRAwUyYms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675907028666,"user_tz":-540,"elapsed":252,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"35b4940d-a698-4745-aa49-f5d8883706f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["18.0\n"]}],"source":["x = tf.constant(3.0)\n","\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = x * x  # y is intermediate value\n","  z = y * y #y**2 -> 2y\n","\n","# Use the tape to compute the gradient of z with respect to the\n","# intermediate value y.\n","# dz_dy = 2 * y and y = x ** 2 = 9\n","print(tape.gradient(z, y).numpy())"]},{"cell_type":"markdown","metadata":{"id":"ISkXuY7YzIcS"},"source":["By default, the resources held by a `GradientTape` are released as soon as the `GradientTape.gradient` method is called. To compute multiple gradients over the same computation, create a gradient tape with `persistent=True`. This allows multiple calls to the `gradient` method as resources are released when the tape object is garbage collected. For example:"]},{"cell_type":"markdown","source":["기본적으로 GradientTape.gradient 메서드가 호출되는 즉시 GradientTape가 보유한 리소스가 해제됩니다. 동일한 계산에 대해 복수의 그라디언트를 계산하려면 Persistent=True인 그라디언트 테이프를 만듭니다. 이렇게 하면 테이프 개체가 garbage collection이 수행될 때 테이프 개체 리소스가 해제되므로 tape.gradient 메서드를 여러 번 호출할 수 있습니다. 예를 들어:"],"metadata":{"id":"J-zLcN5ux2v0"}},{"cell_type":"code","source":["x = tf.constant([1, 3.0])\n","with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = x * x\n","  z = y * y"],"metadata":{"id":"OZt7jokb2RVi","executionInfo":{"status":"ok","timestamp":1675907124993,"user_tz":-540,"elapsed":251,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["print(tape.gradient(z, x).numpy())  # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0])\n","print(tape.gradient(y, x).numpy())  # [2.0, 6.0] (2 * x at x = [1.0, 3.0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"hQA6osdT2UWZ","executionInfo":{"status":"error","timestamp":1675907130372,"user_tz":-540,"elapsed":277,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"71619b12-f12d-4651-c42b-97f70c8aa02e"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["[  4. 108.]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-355c148a3e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [2.0, 6.0] (2 * x at x = [1.0, 3.0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \"\"\"\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n\u001b[0m\u001b[1;32m   1041\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"]}]},{"cell_type":"code","execution_count":33,"metadata":{"id":"zZaCm3-9zVCi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675907149962,"user_tz":-540,"elapsed":236,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"6791bb78-fd57-408e-b763-16b345262003"},"outputs":[{"output_type":"stream","name":"stdout","text":["[  4. 108.]\n","[2. 6.]\n"]}],"source":["x = tf.constant([1, 3.0])\n","with tf.GradientTape(persistent=True) as tape:\n","  tape.watch(x)\n","  y = x * x\n","  z = y * y\n","\n","print(tape.gradient(z, x).numpy())  # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0])\n","print(tape.gradient(y, x).numpy())  # [2.0, 6.0] (2 * x at x = [1.0, 3.0])"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"j8bv_jQFg6CN","executionInfo":{"status":"ok","timestamp":1675907167581,"user_tz":-540,"elapsed":258,"user":{"displayName":"서성원","userId":"15488514127948330387"}}},"outputs":[],"source":["del tape   # Drop the reference to the tape"]},{"cell_type":"markdown","metadata":{"id":"O_ZY-9BUB7vX"},"source":["## Notes on performance\n","\n","* There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n","\n","* Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n","\n","  For efficiency, some ops (like `ReLU`) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use `persistent=True` on your tape, *nothing is discarded* and your peak memory usage will be higher."]},{"cell_type":"markdown","source":["* 그래디언트 테이프 컨텍스트 내에서 Ops을 수행하는 것과 관련된 약간의 오버헤드가 있습니다. 대부분의 eager 실행 모드의 경우, 이는 눈에 띄는 비용은 아니지만 필요한 영역에서만 테이프 컨텍스트를 사용해야 합니다.\n","\n","* 그라디언트 테이프는 backward pass 동안 사용할 입력 및 출력을 포함한 중간 결과를 저장하기 위해 메모리를 사용합니다.</br>\n","효율성을 위해 일부 Ops(예: ReLU)은 중간 결과를 유지할 필요가 없으며 forward pass 중에 정리됩니다. 그러나 테이프에서 Persistent=True를 사용하면 어떤 것도 버리지 않기 때문에 메모리 최대 사용량이 더 높아집니다."],"metadata":{"id":"kS2KE_HW2OSo"}},{"cell_type":"markdown","metadata":{"id":"9dLBpZsJebFq"},"source":["## Gradients of non-scalar targets"]},{"cell_type":"markdown","metadata":{"id":"7pldU9F5duP2"},"source":["A gradient is fundamentally an operation on a scalar."]},{"cell_type":"markdown","source":["gradient는 기본적으로 스칼라에 대한 Ops입니다."],"metadata":{"id":"lmkqYr5B3LB3"}},{"cell_type":"markdown","source":["y = 1/x</br>\n","y = x<sup>-1</sup></br>\n","y = -1x<sup>-2</sup></br>\n","y = -1/x<sup>2</sup></br>"],"metadata":{"id":"EMFwei963jub"}},{"cell_type":"markdown","source":["#### y1 = 1 / x\n","* y1 = x<sup>-1</sup>\n","* y1' = -1x<sup>-2</sup>"],"metadata":{"id":"mUWvMpK5Nl0B"}},{"cell_type":"code","execution_count":35,"metadata":{"id":"qI0sDV_WeXBb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675908244615,"user_tz":-540,"elapsed":330,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"e995004d-0e1d-4ed7-cc68-04973442dea6"},"outputs":[{"output_type":"stream","name":"stdout","text":["4.0\n","-0.25\n"]}],"source":["x = tf.Variable(2.0)\n","with tf.GradientTape(persistent=True) as tape:\n","  y0 = x**2\n","  y1 = 1 / x  \n","\n","print(tape.gradient(y0, x).numpy())\n","print(tape.gradient(y1, x).numpy())"]},{"cell_type":"markdown","metadata":{"id":"COEyYp34fxj4"},"source":["Thus, if you ask for the gradient of multiple targets, the result for each source is:\n","\n","* The gradient of the sum of the targets, or equivalently\n","* The sum of the gradients of each target."]},{"cell_type":"markdown","source":["따라서 여러 타겟의 gradient를 요청하면 각 소스에 대한 결과는 다음과 같습니다.\n","\n","* target 합계의 gradient, 또는 동등하게\n","* 각 target의 gradient의 합계입니다."],"metadata":{"id":"LeXWXLlh3707"}},{"cell_type":"markdown","source":[" 아래 코드에서 non-scalar targets 은 </br>\n"," {'y0': y0, 'y1': y1}를 의미한다."],"metadata":{"id":"tYAFej7tkQC9"}},{"cell_type":"code","execution_count":36,"metadata":{"id":"o4a6_YOcfWKS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675908300395,"user_tz":-540,"elapsed":295,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"5efbf788-3d7d-4f51-fd89-f1e536677499"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.75\n"]}],"source":["x = tf.Variable(2.0)\n","with tf.GradientTape() as tape:\n","  y0 = x**2\n","  y1 = 1 / x\n","\n","print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"]},{"cell_type":"markdown","metadata":{"id":"uvP-mkBMgbym"},"source":["Similarly, if the target(s) are not scalar the gradient of the sum is calculated:"]},{"cell_type":"markdown","source":["마찬가지로, 타겟이 스칼라가 아닌 경우 합계의 기울기가 계산됩니다."],"metadata":{"id":"AEjJdQcZ4z5H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DArPWqsSh5un","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675848479401,"user_tz":-540,"elapsed":697,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"062551ab-7072-4a27-b5e1-9b33d5cf9c91"},"outputs":[{"output_type":"stream","name":"stdout","text":["7.0\n"]}],"source":["x = tf.Variable(2.)\n","\n","with tf.GradientTape() as tape:\n","  y = x * [3., 4.] # [3., 4.]x , y' = [3., 4.]\n","\n","print(tape.gradient(y, x).numpy())"]},{"cell_type":"markdown","metadata":{"id":"flDbx68Zh5Lb"},"source":["This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n","\n","If you need a separate gradient for each item, refer to [Jacobians](advanced_autodiff.ipynb#jacobians)."]},{"cell_type":"markdown","source":["이것은 손실 모음 합계의 기울기 또는 요소별 손실 계산 합계 기울기를 취하는 것을 간단하게 만듭니다.\n","\n","각 항목에 대해 별도의 그래디언트가 필요한 경우 Jacobians를 참조하십시오."],"metadata":{"id":"nzWuO9KH5jlS"}},{"cell_type":"markdown","metadata":{"id":"iwFswok8RAly"},"source":["In some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:"]},{"cell_type":"markdown","source":["경우에 따라 야코비 행렬을 건너뛸 수 있습니다. element-wise 계산의 경우에 합계의 기울기는 각 element가 독립적이므로 입력 요소에 대한 각 element의 도함수를 제공합니다."],"metadata":{"id":"ihajKGAV57Za"}},{"cell_type":"code","execution_count":37,"metadata":{"id":"JQvk_jnMmTDS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675908448348,"user_tz":-540,"elapsed":271,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"c676cc53-9d3e-45b4-e0b4-b3875960bf12"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[-10.          -9.9         -9.8         -9.7         -9.6\n","  -9.5         -9.4         -9.3         -9.2         -9.1\n","  -9.          -8.9         -8.8         -8.7         -8.6\n","  -8.5         -8.4         -8.3         -8.2         -8.1\n","  -8.          -7.8999996   -7.8         -7.7         -7.6\n","  -7.5         -7.3999996   -7.3         -7.2         -7.1\n","  -7.          -6.8999996   -6.8         -6.7         -6.6\n","  -6.5         -6.3999996   -6.3         -6.2         -6.1\n","  -6.          -5.9         -5.7999997   -5.7         -5.6\n","  -5.5         -5.4         -5.2999997   -5.2         -5.1\n","  -5.          -4.9         -4.7999997   -4.7         -4.6\n","  -4.5         -4.4         -4.2999997   -4.2         -4.1\n","  -4.          -3.9         -3.7999997   -3.6999998   -3.6\n","  -3.5         -3.4         -3.2999997   -3.1999998   -3.1\n","  -3.          -2.9         -2.7999997   -2.6999998   -2.6\n","  -2.5         -2.4         -2.2999997   -2.1999998   -2.1\n","  -2.          -1.8999996   -1.8000002   -1.6999998   -1.5999994\n","  -1.5         -1.3999996   -1.3000002   -1.1999998   -1.0999994\n","  -1.          -0.8999996   -0.8000002   -0.6999998   -0.5999994\n","  -0.5         -0.39999962  -0.3000002   -0.19999981  -0.09999943\n","   0.           0.10000038   0.19999981   0.3000002    0.40000057\n","   0.5          0.6000004    0.6999998    0.8000002    0.9000006\n","   1.           1.1000004    1.1999998    1.3000002    1.4000006\n","   1.5          1.6000004    1.6999998    1.8000002    1.9000006\n","   2.           2.1000004    2.1999998    2.3000002    2.4000006\n","   2.5          2.6000004    2.6999998    2.8000002    2.9000006\n","   3.           3.1000004    3.1999998    3.3000002    3.4000006\n","   3.5          3.6000004    3.6999998    3.8000002    3.9000006\n","   4.           4.1000004    4.2          4.3          4.4000006\n","   4.5          4.6000004    4.7          4.8          4.9000006\n","   5.           5.1000004    5.2          5.3          5.4000006\n","   5.5          5.6000004    5.7          5.8          5.9000006\n","   6.           6.1000004    6.200001     6.300001     6.3999996\n","   6.5          6.6000004    6.700001     6.800001     6.8999996\n","   7.           7.1000004    7.200001     7.300001     7.3999996\n","   7.5          7.6000004    7.700001     7.800001     7.8999996\n","   8.           8.1          8.200001     8.300001     8.4\n","   8.5          8.6          8.700001     8.800001     8.9\n","   9.           9.1          9.200001     9.300001     9.4\n","   9.5          9.6          9.700001     9.800001     9.9\n","  10.        ], shape=(201,), dtype=float32)\n"]}],"source":["x = tf.linspace(-10.0, 10.0, 200+1)\n","print(x)"]},{"cell_type":"code","source":["with tf.GradientTape() as tape:\n","  tape.watch(x)\n","  y = tf.nn.sigmoid(x)\n","\n","dy_dx = tape.gradient(y, x)\n","print(dy_dx)"],"metadata":{"id":"zqJrfgWd6L5v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675908474204,"user_tz":-540,"elapsed":390,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"ecffb62e-a918-4b31-9a50-301d0cbe52b9"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[4.53958055e-05 5.01696668e-05 5.54454418e-05 6.12759977e-05\n"," 6.77195421e-05 7.48406237e-05 8.27104086e-05 9.14074990e-05\n"," 1.01019003e-04 1.11640831e-04 1.23379359e-04 1.36351780e-04\n"," 1.50687614e-04 1.66530372e-04 1.84037970e-04 2.03385585e-04\n"," 2.24766336e-04 2.48393306e-04 2.74502818e-04 3.03354871e-04\n"," 3.35237681e-04 3.70468944e-04 4.09399363e-04 4.52417444e-04\n"," 4.99951013e-04 5.52473008e-04 6.10506395e-04 6.74626906e-04\n"," 7.45472440e-04 8.23745097e-04 9.10221192e-04 1.00575748e-03\n"," 1.11129810e-03 1.22788746e-03 1.35667447e-03 1.49892876e-03\n"," 1.65605021e-03 1.82957889e-03 2.02121888e-03 2.23284075e-03\n"," 2.46650935e-03 2.72449688e-03 3.00930650e-03 3.32368701e-03\n"," 3.67066660e-03 4.05357219e-03 4.47605597e-03 4.94213402e-03\n"," 5.45620080e-03 6.02308102e-03 6.64805667e-03 7.33690662e-03\n"," 8.09594616e-03 8.93206056e-03 9.85276420e-03 1.08662304e-02\n"," 1.19813345e-02 1.32077131e-02 1.45557625e-02 1.60367284e-02\n"," 1.76627059e-02 1.94466673e-02 2.14024857e-02 2.35449132e-02\n"," 2.58895941e-02 2.84530222e-02 3.12524661e-02 3.43058892e-02\n"," 3.76317762e-02 4.12490219e-02 4.51766625e-02 4.94335666e-02\n"," 5.40381297e-02 5.90077192e-02 6.43583089e-02 7.01037124e-02\n"," 7.62549937e-02 8.28195885e-02 8.98003429e-02 9.71947163e-02\n"," 1.04993582e-01 1.13180295e-01 1.21729329e-01 1.30605757e-01\n"," 1.39763847e-01 1.49146453e-01 1.58684939e-01 1.68298349e-01\n"," 1.77894458e-01 1.87369928e-01 1.96611941e-01 2.05500335e-01\n"," 2.13909671e-01 2.21712887e-01 2.28784278e-01 2.35003710e-01\n"," 2.40260765e-01 2.44458303e-01 2.47516572e-01 2.49376029e-01\n"," 2.50000000e-01 2.49376029e-01 2.47516587e-01 2.44458303e-01\n"," 2.40260705e-01 2.35003710e-01 2.28784218e-01 2.21712887e-01\n"," 2.13909671e-01 2.05500260e-01 1.96611926e-01 1.87369838e-01\n"," 1.77894443e-01 1.68298304e-01 1.58684835e-01 1.49146438e-01\n"," 1.39763758e-01 1.30605742e-01 1.21729344e-01 1.13180213e-01\n"," 1.04993574e-01 9.71946642e-02 8.98003504e-02 8.28195363e-02\n"," 7.62549862e-02 7.01037124e-02 6.43582866e-02 5.90077341e-02\n"," 5.40381111e-02 4.94335368e-02 4.51766551e-02 4.12489809e-02\n"," 3.76317799e-02 3.43058519e-02 3.12524699e-02 2.84530446e-02\n"," 2.58895643e-02 2.35448945e-02 2.14024913e-02 1.94466617e-02\n"," 1.76627338e-02 1.60366967e-02 1.45557523e-02 1.32076964e-02\n"," 1.19813140e-02 1.08662117e-02 9.85273253e-03 8.93205591e-03\n"," 8.09593033e-03 7.33687775e-03 6.64803293e-03 6.02310384e-03\n"," 5.45621011e-03 4.94212657e-03 4.47605969e-03 4.05359641e-03\n"," 3.67064914e-03 3.32369935e-03 3.00932792e-03 2.72451527e-03\n"," 2.46652518e-03 2.23284843e-03 2.02120445e-03 1.82960229e-03\n"," 1.65604567e-03 1.49894902e-03 1.35666353e-03 1.22789398e-03\n"," 1.11128297e-03 1.00576843e-03 9.10226954e-04 8.23771697e-04\n"," 7.45455211e-04 6.74626499e-04 6.10514835e-04 5.52467944e-04\n"," 4.99952002e-04 4.52432781e-04 4.09375789e-04 3.70484311e-04\n"," 3.35223274e-04 3.03355162e-04 2.74523190e-04 2.48370430e-04\n"," 2.24778167e-04 2.03389267e-04 1.84025266e-04 1.66507642e-04\n"," 1.50657841e-04 1.36356830e-04 1.23366393e-04 1.11627036e-04\n"," 1.01019665e-04 9.14251650e-05 8.27244003e-05 7.48578314e-05\n"," 6.77062926e-05 6.12698204e-05 5.54292455e-05 5.01845934e-05\n"," 4.54166766e-05], shape=(201,), dtype=float32)\n"]}]},{"cell_type":"code","source":["sdev = tf.nn.sigmoid(x)*(1-tf.nn.sigmoid(x))  # A(1-A)\n","print(sdev)"],"metadata":{"id":"f5Vc7guxSYdm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675848571945,"user_tz":-540,"elapsed":527,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"8b097dc1-9d27-4106-8272-10b161103683"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[4.53958055e-05 5.01696704e-05 5.54454418e-05 6.12759977e-05\n"," 6.77195349e-05 7.48406237e-05 8.27104013e-05 9.14074917e-05\n"," 1.01019010e-04 1.11640838e-04 1.23379359e-04 1.36351780e-04\n"," 1.50687600e-04 1.66530357e-04 1.84037955e-04 2.03385600e-04\n"," 2.24766321e-04 2.48393306e-04 2.74502789e-04 3.03354813e-04\n"," 3.35237681e-04 3.70468944e-04 4.09399334e-04 4.52417444e-04\n"," 4.99951013e-04 5.52473066e-04 6.10506453e-04 6.74626906e-04\n"," 7.45472440e-04 8.23745097e-04 9.10221133e-04 1.00575760e-03\n"," 1.11129810e-03 1.22788735e-03 1.35667447e-03 1.49892876e-03\n"," 1.65605021e-03 1.82957877e-03 2.02121865e-03 2.23284075e-03\n"," 2.46650912e-03 2.72449665e-03 3.00930603e-03 3.32368701e-03\n"," 3.67066660e-03 4.05357173e-03 4.47605597e-03 4.94213449e-03\n"," 5.45620080e-03 6.02308055e-03 6.64805667e-03 7.33690569e-03\n"," 8.09594523e-03 8.93206056e-03 9.85276420e-03 1.08662294e-02\n"," 1.19813355e-02 1.32077113e-02 1.45557625e-02 1.60367284e-02\n"," 1.76627059e-02 1.94466654e-02 2.14024857e-02 2.35449132e-02\n"," 2.58895941e-02 2.84530222e-02 3.12524661e-02 3.43058892e-02\n"," 3.76317725e-02 4.12490219e-02 4.51766588e-02 4.94335666e-02\n"," 5.40381297e-02 5.90077192e-02 6.43583015e-02 7.01037124e-02\n"," 7.62550011e-02 8.28195885e-02 8.98003429e-02 9.71947163e-02\n"," 1.04993582e-01 1.13180295e-01 1.21729314e-01 1.30605757e-01\n"," 1.39763847e-01 1.49146467e-01 1.58684939e-01 1.68298349e-01\n"," 1.77894473e-01 1.87369928e-01 1.96611941e-01 2.05500335e-01\n"," 2.13909686e-01 2.21712902e-01 2.28784278e-01 2.35003710e-01\n"," 2.40260750e-01 2.44458318e-01 2.47516587e-01 2.49376029e-01\n"," 2.50000000e-01 2.49376029e-01 2.47516572e-01 2.44458303e-01\n"," 2.40260720e-01 2.35003710e-01 2.28784218e-01 2.21712887e-01\n"," 2.13909701e-01 2.05500260e-01 1.96611956e-01 1.87369838e-01\n"," 1.77894413e-01 1.68298334e-01 1.58684835e-01 1.49146467e-01\n"," 1.39763758e-01 1.30605787e-01 1.21729299e-01 1.13180213e-01\n"," 1.04993626e-01 9.71947089e-02 8.98003057e-02 8.28195363e-02\n"," 7.62549862e-02 7.01037124e-02 6.43582866e-02 5.90077341e-02\n"," 5.40381111e-02 4.94335368e-02 4.51766551e-02 4.12490331e-02\n"," 3.76317799e-02 3.43058519e-02 3.12524140e-02 2.84530446e-02\n"," 2.58896220e-02 2.35448945e-02 2.14025490e-02 1.94466058e-02\n"," 1.76627338e-02 1.60367545e-02 1.45557523e-02 1.32076964e-02\n"," 1.19813727e-02 1.08662117e-02 9.85279121e-03 8.93211458e-03\n"," 8.09593033e-03 7.33687775e-03 6.64803293e-03 6.02304516e-03\n"," 5.45615098e-03 4.94218525e-03 4.47605969e-03 4.05353727e-03\n"," 3.67064914e-03 3.32369935e-03 3.00932792e-03 2.72445590e-03\n"," 2.46646581e-03 2.23290781e-03 2.02120445e-03 1.82960229e-03\n"," 1.65604567e-03 1.49894902e-03 1.35672290e-03 1.22795347e-03\n"," 1.11128297e-03 1.00576843e-03 9.10167466e-04 8.23712209e-04\n"," 7.45514699e-04 6.74626499e-04 6.10574323e-04 5.52527432e-04\n"," 4.99952002e-04 4.52492328e-04 4.09375789e-04 3.70484311e-04\n"," 3.35223274e-04 3.03295586e-04 2.74523190e-04 2.48430006e-04\n"," 2.24718591e-04 2.03389267e-04 1.83965676e-04 1.66507642e-04\n"," 1.50657841e-04 1.36356830e-04 1.23366393e-04 1.11686626e-04\n"," 1.01079262e-04 9.14251650e-05 8.27244003e-05 7.48578314e-05\n"," 6.77062926e-05 6.12698204e-05 5.54292455e-05 5.01845934e-05\n"," 4.54166766e-05], shape=(201,), dtype=float32)\n"]}]},{"cell_type":"code","execution_count":39,"metadata":{"id":"e_f2QgDPmcPE","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1675908531064,"user_tz":-540,"elapsed":824,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"06e91f68-4f42-426e-9d86-a7cf84dcaac8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["plt.plot(x, y, label='y')\n","plt.plot(x, dy_dx, label='dy/dx')\n","plt.legend()\n","_ = plt.xlabel('x')"]},{"cell_type":"markdown","metadata":{"id":"6kADybtQzYj4"},"source":["## Control flow\n","\n","Because a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, `if` and `while` statements).\n","\n","Here a different variable is used on each branch of an `if`. The gradient only connects to the variable that was used:"]},{"cell_type":"markdown","source":["그래디언트 테이프는 실행되는 Ops을 기록하기 때문에 Python 제어 흐름이 자연스럽게 처리됩니다(예: if 및 while 문).\n","\n","여기서 if의 각 분기마다 다른 변수가 사용됩니다. 그래디언트는 사용된 변수에만 연결됩니다."],"metadata":{"id":"iBxyaKFK6-_6"}},{"cell_type":"code","execution_count":40,"metadata":{"id":"ciFLizhrrjy7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675908621633,"user_tz":-540,"elapsed":251,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"841d6d47-22d6-43fc-a2f8-53e9273805d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["dv0= tf.Tensor(1.0, shape=(), dtype=float32)\n","dv1= None\n"]}],"source":["x = tf.constant(1.0)\n","\n","v0 = tf.Variable(2.0)\n","v1 = tf.Variable(2.0)\n","\n","with tf.GradientTape(persistent=True) as tape:\n","  tape.watch(x)\n","  if x > 0.0:\n","    result = v0\n","  else:\n","    result = v1**2 \n","\n","dv0, dv1 = tape.gradient(result, [v0, v1])\n","\n","print(\"dv0=\", dv0)\n","print(\"dv1=\", dv1)"]},{"cell_type":"markdown","metadata":{"id":"HKnLaiapsjeP"},"source":["Just remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n","\n","Depending on the value of `x` in the above example, the tape either records `result = v0` or `result = v1**2`. The gradient with respect to `x` is always `None`."]},{"cell_type":"markdown","source":["제어문 자체는 미분할 수 없으므로 그래디언트 기반 옵티마이저에서는 볼 수 없습니다.\n","\n","위의 예에서 x 값에 따라 테이프는 `result = v0` 또는 `result = v1**2`를 기록합니다. x에 대한 기울기는 항상 없음입니다."],"metadata":{"id":"yaOlf3J-7uz1"}},{"cell_type":"code","execution_count":41,"metadata":{"id":"8k05WmuAwPm7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675908664951,"user_tz":-540,"elapsed":260,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"a93a35d8-c33a-4a29-e1cb-f3788e06f48d"},"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}],"source":["dx = tape.gradient(result, x)\n","\n","print(dx)"]},{"cell_type":"markdown","metadata":{"id":"egypBxISAHhx"},"source":["## Cases where `gradient` returns `None`\n","\n","When a target is not connected to a source, `gradient` will return `None`.\n"]},{"cell_type":"markdown","source":["target이 source에 연결되어 있지 않으면 그라디언트는 `None`을 반환합니다."],"metadata":{"id":"hw2UHoWS78uW"}},{"cell_type":"code","execution_count":42,"metadata":{"id":"CU185WDM81Ut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675908732572,"user_tz":-540,"elapsed":350,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"35794b55-7255-4a2f-ba2c-20a6fa63b3cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}],"source":["x = tf.Variable(2.)\n","y = tf.Variable(3.)\n","\n","with tf.GradientTape() as tape:\n","  z = y * y\n","  \n","print(tape.gradient(z, x))"]},{"cell_type":"markdown","metadata":{"id":"sZbKpHfBRJym"},"source":["Here `z` is obviously not connected to `x`, but there are several less-obvious ways that a gradient can be disconnected."]},{"cell_type":"markdown","source":["여기서 z는 분명히 x에 연결되어 있지 않지만 그래디언트 연결을 끊을 수 있는 덜 분명한 방법이 몇 가지 있습니다."],"metadata":{"id":"cqFkFhcj8JSP"}},{"cell_type":"markdown","metadata":{"id":"eHDzDOiQ8xmw"},"source":["### 1. Replaced a variable with a tensor\n","\n","In the section on [\"controlling what the tape watches\"](#watches) you saw that the tape will automatically watch a `tf.Variable` but not a `tf.Tensor`.\n","\n","One common error is to inadvertently replace a `tf.Variable` with a `tf.Tensor`, instead of using `Variable.assign` to update the `tf.Variable`. Here is an example:"]},{"cell_type":"markdown","source":["\"테이프 감시 대상 제어\" 섹션에서 테이프가 자동으로 tf.Variable을 감시하지만 tf.Tensor는 감시하지 않는 것을 보았습니다.\n","\n","일반적인 오류 중 하나는 **Variable.assign을 사용하여 tf.Variable을 업데이트하는 대신 실수로 tf.Variable을 tf.Tensor로 바꾸는 것**입니다. 다음은 예입니다.(무시하세요)"],"metadata":{"id":"6RDIFuvefAcy"}},{"cell_type":"code","execution_count":43,"metadata":{"id":"QPKY4Tn9zX7_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675908794971,"user_tz":-540,"elapsed":264,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"af7aceca-e7cc-44b8-9d46-3f52548358b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32) \n","\n","ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32) \n","\n"]}],"source":["x = tf.Variable(2.0)\n","\n","for epoch in range(2):\n","  with tf.GradientTape() as tape:\n","    y = x+1\n","    # print(y)\n","\n","  print(type(x).__name__, \":\", tape.gradient(y, x), \"\\n\")\n","  \n","  "]},{"cell_type":"code","source":["x = tf.Variable(2.0)\n","\n","for epoch in range(2):\n","  with tf.GradientTape() as tape:\n","    y = x.assign_add(1)\n","    # print(y)\n","\n","  print(type(x).__name__, \":\", tape.gradient(y, x), \"\\n\")  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDU7RDNlYSCY","executionInfo":{"status":"ok","timestamp":1675908894590,"user_tz":-540,"elapsed":234,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"f30ba697-cba1-41e2-eaf6-f0fe88a24e10"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32) \n","\n","ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32) \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"3gwZKxgA97an"},"source":["### 2. Did calculations outside of TensorFlow\n","\n","The tape can't record the gradient path if the calculation exits TensorFlow.\n","For example:"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"jmoLCDJb_yw1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675909052518,"user_tz":-540,"elapsed":254,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"e7a34c67-b1d4-44dd-c907-e2f115775b0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tape.gradient(y, x)= None\n"]}],"source":["x = tf.Variable([[1.0, 2.0],\n","                 [3.0, 4.0]], dtype=tf.float32)\n","\n","with tf.GradientTape() as tape:\n","  x2 = x**2\n","  # print(\"x2=\", x2, \"\\n\")\n","  # This step is calculated with NumPy\n","  y = tf.convert_to_tensor(np.mean(x2, axis=0))  # no\n","  # y = tf.convert_to_tensor(tf.reduce_mean(x2, axis=0)) #yes\n","  # print(\"mean of x2=\", y, \"\\n\")\n","  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n","  # using `tf.convert_to_tensor`.\n","  y = tf.convert_to_tensor(tf.reduce_mean(y, axis=0))\n","  # print(\"y=\", y, \"\\n\")\n","\n","print(\"tape.gradient(y, x)=\", tape.gradient(y, x))"]},{"cell_type":"markdown","metadata":{"id":"p3YVfP3R-tp7"},"source":["### 3. Took gradients through an integer or string\n","\n","Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n","\n","Nobody expects strings to be differentiable, but it's easy to accidentally create an `int` constant or variable if you don't specify the `dtype`."]},{"cell_type":"markdown","source":["정수와 문자열은 미분할 수 없습니다. 계산 경로가 이러한 데이터 유형을 사용하는 경우 기울기가 없습니다.\n","\n","아무도 문자열을 미분할 수 있다고 기대하지 않지만 dtype을 지정하지 않으면 실수로 int 상수나 변수를 생성하기 쉽습니다."],"metadata":{"id":"dnIyomGNJs7W"}},{"cell_type":"code","execution_count":50,"metadata":{"id":"9jlHXHqfASU3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675909094794,"user_tz":-540,"elapsed":256,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"86e1df38-cce9-4cb2-c49b-8d8d6d7c1c4b"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n","WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n","WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"]},{"output_type":"stream","name":"stdout","text":["None\n"]}],"source":["x = tf.constant(10) # integer type\n","\n","with tf.GradientTape() as g:\n","  g.watch(x)\n","  y = x * x\n","\n","print(g.gradient(y, x))"]},{"cell_type":"markdown","metadata":{"id":"RsdP_mTHX9L1"},"source":["TensorFlow doesn't automatically cast between types, so, in practice, you'll often get a type error instead of a missing gradient."]},{"cell_type":"markdown","metadata":{"id":"WyAZ7C8qCEs6"},"source":["### 4. Took gradients through a stateful object\n","\n","State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n","\n","A `tf.Tensor` is immutable. You can't change a tensor once it's created. It has a _value_, but no _state_. All the operations discussed so far are also stateless: the output of a `tf.matmul` only depends on its inputs.\n","\n","A `tf.Variable` has internal state—its value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:\n"]},{"cell_type":"markdown","source":["[참고](https://stackoverflow.com/questions/52636943/what-is-a-stateful-object-in-tensorflow)"],"metadata":{"id":"ALNB-Wojei9l"}},{"cell_type":"markdown","source":["`State`는 그라디언트를 중지시킵니다. `stateful object`에서 값을 read할 때 테이프는 현재 `state`만 관찰할 수 있으며 현재 `state`로 이어지는 `history`은 관찰할 수 없습니다.\n","\n","tf.Tensor는 변경할 수 없습니다. 텐서는 생성된 후에는 변경할 수 없습니다. 값은 있지만 state는 없습니다. 지금까지 논의된 모든 ops도 state를 저장하지 않습니다: tf.matmul의 출력은 입력에만 의존합니다.\n","\n","tf.Variable에는 내부 state, 즉 값이 있습니다. 변수를 사용할 때, `state`를 read 합니다. 변수에 대한 기울기를 계산하는 것은 정상이지만, tf.Variable의 `State`는 그래디언트 계산이 더 뒤로 가지 않도록 차단(`x1.assign_add(x0)`)합니다. 예를 들어"],"metadata":{"id":"zrJg_JXlKZoY"}},{"cell_type":"code","execution_count":51,"metadata":{"id":"C1tLeeRFE479","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675909294969,"user_tz":-540,"elapsed":252,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"c681fc52-4729-4886-8bb1-8afb5d94ce52"},"outputs":[{"output_type":"stream","name":"stdout","text":["tape.gradient(y, x0)= None\n"]}],"source":["x0 = tf.Variable(3.0)\n","x1 = tf.Variable(0.0)\n","\n","with tf.GradientTape(persistent=True) as tape:\n","  # Update x1 = x1 + x0.\n","  x1.assign_add(x0) # not record\n","  # The tape starts recording from x1.\n","  y = x1**2   # y = (x1 + x0)**2\n","\n","# This doesn't work.\n","print(\"tape.gradient(y, x0)=\", tape.gradient(y, x0))   \n","\n","# This work.\n","# print(\"tape.gradient(y, x1)=\", tape.gradient(y, x1))   "]},{"cell_type":"code","source":["x0 = tf.Variable(3.0)\n","x1 = tf.Variable(0.0)\n","\n","with tf.GradientTape(persistent=True) as tape:\n","  # Update x1 = x1 + x0.\n","  x1 = x1 + x0  # record\n","  # The tape starts recording from x1.\n","  y = x1**2   # y = (x1 + x0)**2 -> \n","\n","# This work.\n","print(\"tape.gradient(y, x0)=\", tape.gradient(y, x0))   \n","\n","# This work.\n","# print(\"tape.gradient(y, x1)=\", tape.gradient(y, x1))   "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lZOYYjBz4_NE","executionInfo":{"status":"ok","timestamp":1675909575350,"user_tz":-540,"elapsed":247,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"1920c55e-8981-498e-e007-fb604a7101c8"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["tape.gradient(y, x0)= tf.Tensor(6.0, shape=(), dtype=float32)\n"]}]},{"cell_type":"markdown","metadata":{"id":"xKA92-dqF2r-"},"source":["Similarly, `tf.data.Dataset` iterators and `tf.queue`s are stateful, and will stop all gradients on tensors that pass through them."]},{"cell_type":"markdown","metadata":{"id":"HHvcDGIbOj2I"},"source":["## No gradient registered"]},{"cell_type":"markdown","metadata":{"id":"aoc-A6AxVqry"},"source":["Some `tf.Operation`s are **registered as being non-differentiable** and will return `None`. Others have **no gradient registered**.\n","\n","The `tf.raw_ops` page shows which low-level ops have gradients registered.\n","\n","If you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning `None`. This way you know something has gone wrong.\n","\n","For example, the `tf.image.adjust_contrast` function wraps `raw_ops.AdjustContrastv2`, which could have a gradient but the gradient is not implemented:\n"]},{"cell_type":"markdown","source":["일부 tf.Operations는 미분할 수 없는 것으로 등록되어 None을 반환합니다. 다른 것들은 등록된 그라디언트가 없습니다.\n","\n","[tf.raw_ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops) 페이지는 등록된 그라디언트가 있는 low-level ops을 보여줍니다.\n","\n","등록된 그라디언트가 없는 float 연산을 통해 그라디언트를 사용하려고 하면 테이프에서 자동으로 None을 반환하는 대신 오류가 발생합니다. 이렇게 하면 문제가 발생했음을 알 수 있습니다.\n","\n","예를 들어, tf.image.adjust_contrast 함수는 그라디언트가 있을 수 있지만 그라디언트가 구현되지 않은 raw_ops.AdjustContrastv2를 래핑합니다."],"metadata":{"id":"EQu2aM3aHlGy"}},{"cell_type":"code","execution_count":54,"metadata":{"id":"HSb20FXc_V0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675910421207,"user_tz":-540,"elapsed":328,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"465d5759-d2cb-4329-9a8b-bbdb46926f5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["LookupError: gradient registry has no entry for: AdjustContrastv2\n"]}],"source":["image = tf.Variable([[[0.5, 0.0, 0.0]]])\n","delta = tf.Variable(0.1)\n","\n","with tf.GradientTape() as tape:\n","  new_image = tf.image.adjust_contrast(image, delta)\n","\n","try:\n","  print(tape.gradient(new_image, [image, delta]))\n","  # assert False   # This should not happen.\n","except LookupError as e:\n","  print(f'{type(e).__name__}: {e}')\n"]},{"cell_type":"markdown","metadata":{"id":"pDoutjzATiEm"},"source":["If you need to differentiate through this op, you'll either need to implement the gradient and register it (using `tf.RegisterGradient`) or re-implement the function using other ops."]},{"cell_type":"markdown","metadata":{"id":"GCTwc_dQXp2W"},"source":["## Zeros instead of None"]},{"cell_type":"markdown","metadata":{"id":"TYDrVogA89eA"},"source":["In some cases it would be convenient to get 0 instead of `None` for unconnected gradients.  You can decide what to return when you have unconnected gradients using the `unconnected_gradients` argument:"]},{"cell_type":"markdown","source":["어떤 경우에는 연결되지 않은 그라디언트에 대해 None 대신 0을 얻는 것이 편리합니다. unconnected_gradients 인수를 사용하여 연결되지 않은 그라디언트가 있을 때 반환할 항목을 결정할 수 있습니다."],"metadata":{"id":"WX3BftjoJh6e"}},{"cell_type":"code","execution_count":55,"metadata":{"id":"U6zxk1sf9Ixx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675910528880,"user_tz":-540,"elapsed":297,"user":{"displayName":"서성원","userId":"15488514127948330387"}},"outputId":"6ff1e85d-0ac1-4ce0-b51f-db983e5912e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"]}],"source":["x = tf.Variable([2., 2.])\n","y = tf.Variable(3.)\n","\n","with tf.GradientTape() as tape:\n","  z = y**2\n","  \n","print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"]}],"metadata":{"colab":{"collapsed_sections":["Tce3stUlHN0L"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}